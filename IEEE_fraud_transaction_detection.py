# -*- coding: utf-8 -*-
"""CSE519_hw2_Brahmavar_Srinidhi_112584858.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K0k8br9PQKnKw_7GxrlZawijxDFLttXb

# Homework 2 - IEEE Fraud Detection

For all parts below, answer all parts as shown in the Google document for Homework 2. Be sure to include both code that justifies your answer as well as text to answer the questions. We also ask that code be commented to make it easier to follow.

# Part - 0 - Preprocessing
"""

#This section contains all the import statements required for this challenge  
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math


#this section also contains common code/pre-processing which may be used in all the below cells

#loading the training files 
train_transaction = pd.read_csv("/content/drive/My Drive/DSF_Data/train_transaction.csv")
train_identity = pd.read_csv("/content/drive/My Drive/DSF_Data/train_identity.csv")

#loading the testing csv file 
test_transaction = pd.read_csv("/content/drive/My Drive/DSF_Data/test_transaction.csv")
test_identity = pd.read_csv("/content/drive/My Drive/DSF_Data/test_identity.csv")

"""
As explained that the attributes are split in two tables, we need to join the tables
and then we can continue.
"""
#merging of the two tables
#explanation of the attributes
train_total = pd.merge(train_transaction,train_identity,on='TransactionID',how='left')
test_total = pd.merge(test_transaction,test_identity,on='TransactionID',how ='left')

#making datadrame with required attributes for problems 1-5, will be faster to work and process with.
df = train_total[['TransactionID','isFraud','DeviceType','DeviceInfo','TransactionDT','TransactionAmt','ProductCD','card4','card6','P_emaildomain','R_emaildomain','addr1','addr2','dist1','dist2']]

"""This part of the code is used for common pre-processing. It contains the common modules needed for questions 1-5. 
We read the csv files into Panda dataframes and then join the train (transaction and identity) and test (transaction and identity) as attributes are split amongst there in the dataset.
The primary key is TransactionID and since it is there in both the tables, we have used as the common attribute for join. 

As the Homework document suggested we will be needing a set of 15 attributes throught question 1-5 and so a smaller dataframe was made using just there attributes. This will reduce the runtime memory requirment throught the usage.

## Part 1 - Fraudulent vs Non-Fraudulent Transaction
"""

"""
As per sheet TODO task is:
Filter out your data to examine just the fraudulent transactions. For each field above,
examine the distribution of the values, and explain any interesting insight you get
from this. How do do the distributions on fraudulent transactions compare to the nonfraudulent
ones? (15 points)
"""
df_fraud = df.loc[df['isFraud'] == 1]
df_normal = df.loc[df['isFraud'] == 0]


fraud_total = df_fraud['TransactionAmt'].sum()
normal_total = df_normal['TransactionAmt'].sum()


fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))
df_fraud['TransactionAmt'].apply(np.log).plot(kind='hist',
          bins=100,
          title='Log Transaction Amt - Fraud',
          color="green",
          xlim=(-3, 10),
         ax= ax1)


df_normal['TransactionAmt'].apply(np.log).plot(kind='hist',
          bins=100,
          title='Log Transaction Amt - Not Fraud',
          color="blueviolet",
          xlim=(-3, 10),
         ax=ax2)

df_fraud['TransactionAmt'].plot(kind='hist',
          bins=100,
          title='Transaction Amt - Fraud',
          color="green",
         ax= ax3)

df_normal['TransactionAmt'].plot(kind='hist',
          bins=100,
          title='Transaction Amt - Not Fraud',
          color="blueviolet",
         ax=ax4)
plt.show()


fig, axes = plt.subplots(3,1, figsize = (10,7))
fig.suptitle("Fraud transactions v/s Some of the other attributes in the Data Set")

#distribution of fraud vs non fraud
fraud = sns.countplot(x = 'isFraud',data =train_total, ax = axes[0])
#fraud.set_axis_labels('count','Type of Transaction')
#types of cards which are involved in fraud
card4 = sns.countplot(x = 'card4', data = df_fraud,ax = axes[1])
#
DeviceType = sns.countplot(x='DeviceType', data=df_fraud, ax=axes[2])
#

"""**Insights**

Axes -1:
Axes 1 compares the frequency of fraud transactions to the amount. 
A comaprission has been done in the log scale and in the absolute scale for both fraud and non-fraud data. 
If we look into it closely, the frequency v/s amount graph is very skewed in the sense that there are a good number of transactions with very high or very low values. So visually not much can be understood out of it. 
Hence to reduce the skewness and get a better understanding of the data, I have applied log and then plotted it with transations. 
After applying log, the skewness has reduced to some extent and we can make out how the transaction frequency are with respect to each other as a multiplicative factor.

## Part 2 - Transaction Frequency
"""

# TODO: code to generate the frequency graph
"""
TODO as per question sheet is : 
The addr2 field gives a code (but not name) associated with the country of the
purchaser. TransactionDT shows the time passed from some reference for each
transaction. By looking at the time of day of the transactions, we can infer what
waking hours are associated with the country relative to the reference time. Analyze
the frequency distribution of transactions by time for the most frequent country code,
as per the addr2 field. Plot this distribution. Explain your findings. (15 points)

As per the kaggle documentation and competation author - addr2 as billing country

"""
import math 
df_trans_freq = df[['addr2','TransactionDT','TransactionAmt']]
#finding the most frequent country code
most_freq_country = df_trans_freq['addr2'].value_counts().idxmax()


#now we have to scale down the time of the day. Since the data is a collection time spans throught months
#we also have to do Mod 24, to get it down to day to day date basis.
#store that in the Dataframe again without changing the current attributes.
df_trans_freq['scaledTime'] = (df_trans_freq['TransactionDT']//(60*60))
df_trans_freq['scaledTime'] = df_trans_freq['scaledTime'] %24
train_transaction['scaledTime'] = df_trans_freq['scaledTime'] %24

#making a new column if df dataframe in case this normalised data is required in the upcoming tasks.
df['scaledTime'] = df_trans_freq['scaledTime']

df_trans_freq[df_trans_freq.addr2 == most_freq_country]['scaledTime'].value_counts().sort_index().plot.bar()

df_coerr = train_transaction.groupby('scaledTime').sum().reset_index(inplace=False)
print("Corelation between transaction amount and Hour of the Day is :",df_coerr['scaledTime'].corr(df_coerr['TransactionAmt']))

"""**Insights**

-> To get a better insight, we converted the time sequence from seconds to hours. Since it was the case that a lot of transactions occured at the same second or within seconds of each other,it would be very hard to visualise. Hence the precesion of seconds or even minutes is not a good one. Hence we have binned it to hours. The hour bin is large enough to store a large number of transactions. 

-> As per the graph the "19th" hour was when the largest amount of sales took place. 

-> It seems like most of the shopping traffic was in the early morning(near midnight)/ late night period. As one would expect  because at early morning people might have a busy schedule and have errands to run to start the day and hence majority of the people found time or leisure only in the second half of the day after work.

## Part 3 - Product Code
"""

# TODO: code to analyze prices for different product codes
"""
TODO as per question sheet is :
ProductCD refers to a product code. Make your best educated guess on which
codes correspond to the most expensive products and which to the cheapest
products. Justify with analysis. (10 points)
"""

df_x = train_transaction['ProductCD']
ax = sns.countplot(df_x)
plt.xticks(rotation = 45)
plt.xlabel('Count')
plt.ylabel('Total')

htype = train_transaction.loc[train_transaction['ProductCD'] == 'H']
ctype = train_transaction.loc[train_transaction['ProductCD'] == 'C']
stype = train_transaction.loc[train_transaction['ProductCD'] == 'S']
rtype = train_transaction.loc[train_transaction['ProductCD'] == 'R']
wtype = train_transaction.loc[train_transaction['ProductCD'] == 'W']

hSum = htype['TransactionAmt'].sum()
cSum = ctype['TransactionAmt'].sum()
sSum = stype['TransactionAmt'].sum()
rSum = rtype['TransactionAmt'].sum()
wSum = wtype['TransactionAmt'].sum()

h_avg = hSum/len(htype)
c_avg = cSum/len(ctype)
s_avg = sSum/len(stype)
r_avg = rSum/len(rtype)
w_avg = wSum/len(wtype)

#Median statistics:
wMed = wtype.loc[:,"TransactionAmt"].median()
hMed = htype.loc[:,"TransactionAmt"].median()
cMed = ctype.loc[:,"TransactionAmt"].median()
rMed = rtype.loc[:,"TransactionAmt"].median()
sMed = stype.loc[:,"TransactionAmt"].median()

print("The mean of W is %f and the median is %f" %(w_avg,wMed))
print("The mean of H is %f and the median is %f" %(h_avg,hMed))
print("The mean of C is %f and the median is %f" %(c_avg,cMed))
print("The mean of S is %f and the median is %f" %(s_avg,sMed))
print("The mean of R is %f and the median is %f" %(r_avg,rMed))

"""**Insights**

From the data above, we can see that it is highly imbalanced. 

-> In my opinion, R is the most expensive products. Why - As we can see that even though R is less in quantity compared to many categories, the total amount spent on 'R' category is the highest. 

Moreover the mean of amount of category R is the highest, which means when sorted the middle transactions are highest in category R. 

-> In my opinion, C is the cheapest category. 
The reasons being that the mean of the transaction amount is the least and also the total of the amount is the least, and also the number of products are not that high.

## Part 4 - Correlation Coefficient
"""

# TODO: code to calculate correlation coefficient
"""
TODO as per sheet is:
Plot the distribution between the time of day and the purchase amount. What is the
correlation coefficient? Note that some cleaning is necessary to get a meaningful
time of day. (10 points)
"""

#first converting time frame to number of days
#days = df_trans_freq['TransactionDT'] //(60*60*24)

#Limiting Days to 7, to signify, days of the week
#limit_days = (days - 1)%7 

list1= df['TransactionAmt'].values
list2 = df['scaledTime'].values

time_of_day = []
purchase_amount =[]

for j in range(len(list1)):
  time_of_day.append(list2[j])
  purchase_amount.append(list1[j])
  
print("The corelation between purchase amount and time of day is :",np.corrcoef([purchase_amount,time_of_day]))



x = df_coerr['scaledTime']
y = df_coerr['TransactionAmt']

#method to get day.
#df['dayOfWeek'] = df_trans_freq['TransactionDT']//(60*60*24)%7
plt.scatter(x,y,c="green", alpha=0.6)
plt.title('Time of Day vs Purchase amount plot')
plt.xlabel('Hour of the Day')
plt.ylabel('Transaction Amount')
plt.show()

"""**Insights**

->The corelation value between the time of the day and purchase amount is 0.048 which shows that there is no much co-relation between the two. 
Hence there is weak corelation between time of the day and purchase amount. 

->This could be because of the fact that different kind of people are active at different times of the day. Some people are active in the mornings and browse the web and do their shopping then, and some people do it at the other times of the day, say at night. 

->The corelation between the entire sum of transactions and the hour in which the transaction took place have a good corelation which is 0.64 
This could attribute to the fact that there is some corelation between these two attributes. Like explained previously, a lot of people might prefer to make their transactions in particular time of the day (say after working hours or after classes/school) and hence this corelation exists.

->The scatter plot between time of the day and amount spent looks very familiar with the transaction frequency plot as shown in question-2 and it graphically shows that there is some corelation between time of the day and the total amount of transaction which has taken place in that hour.

## Part 5 - Interesting Plot
"""

# TODO: code to generate the plot here.
"""
TODO as per the question sheet is:
Create a plot of your own using the dataset that you think reveals something very
interesting. Explain what it is, and anything else you learned. (15 points)
"""

df_cardData = df[['TransactionID','isFraud','card4']]
df_card_type = pd.pivot_table(data = df_cardData, index ='card4',columns = 'isFraud', aggfunc = 'count')
df_card_type['perc'] = (df_card_type.TransactionID[1]/df_card_type.TransactionID[0])*100

plt.figure()
df_card_type.perc.plot(kind  = 'bar', title = 'Percentage of Fradulent transactions by card type')
plt.xlabel('Card Type')

#percentage of cards being used



plt.figure(figsize=(10,5))
ax=sns.countplot(df_fraud['P_emaildomain'])
plt.xticks(rotation = 45)
plt.xlabel('P_emaildomain')
plt.ylabel('Total IDs')
plt.title('P_emaildomain given fraud transaction')

fraudEmailPercentage = df_fraud['P_emaildomain'].value_counts()
actualEmailPercentage = df['P_emaildomain'].value_counts()

"""**Insights**

Interesting about plot 1 -

This plot shows the percentage of cardType which have been used for fraudulent transactions. 
Based on the plot, it is discover credit card. 
Discover is also the most bought card. 
It could also be that the card doesn't have the best security and authentcation. 

The top companies can look into it and make the security parameters much stronger, so that the card cannot be used easily for fraud transactions. 


Interesting about plot 2- 


The Fraud vs Email plot shows us that some of the prominant domain names which have fraud are Hotmail, Gmail and Outlook. The more intresting thing is that these email ID's which were used for fraud is a much lower percentage of the total. Websistes such as sbcglobal.net have a much higher percentage of fraud. 


Still, many be prominant companies which maintain these email domains can do somethiing from their end to make account creation more hard and more tracable so that it is not used for fraud. 
A list is printed below showing the percentage of email ID's used for fraud transactions.

## Part 6 - Prediction Model
"""

# TODO: code for your final model
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score


sample_submission = pd.read_csv('/content/drive/My Drive/DSF_Data/sample_submission.csv')
columns_toDrop = []
num_of_rows = train_transaction.shape[0]
for i in train_transaction.columns:
  count_of_null_vals = train_transaction[i].isna().sum()
  if count_of_null_vals >= num_of_rows//2:
    columns_toDrop.append(i)
#to drop such columns 
train_transaction.drop(columns_toDrop,axis = 1,inplace = True)
test_transaction.drop(columns_toDrop,axis = 1, inplace = True)


#list of numeric columns with missing values 
all_numeric_col = train_transaction.select_dtypes(include=np.number).columns
numeric_missing = []
for i in all_numeric_col:
  missing = train_transaction[i].isna().sum()
  if missing >0:
    numeric_missing.append(i)

#filling missing values with median 
for i in numeric_missing:
  train_transaction[i].fillna(train_transaction[i].median(),inplace = True)
  test_transaction[i].fillna(test_transaction[i].median(),inplace = True)

  
object_columns = train_transaction.select_dtypes(include=object).columns

for i in object_columns:
  lbl = LabelEncoder()
  lbl.fit(list(train_transaction[i].values) + list(test_transaction[i].values))
  
  train_transaction[i] = lbl.transform(list(train_transaction[i].values))
  test_transaction[i] = lbl.transform(list(test_transaction[i].values))

#Need to drop some features, as all features may not contribute towards the classification task
#leaving some space for this task. 
Y_train = train_transaction['isFraud'].copy()
X_train = train_transaction.drop('isFraud',axis = 1)
X_test = test_transaction.copy()



X_train, X_val, y_train , y_val = train_test_split(X_train,Y_train,test_size = 0.4, random_state=0)
m = RandomForestClassifier(n_jobs=-1, n_estimators = 200,criterion='entropy',max_features='auto',min_samples_leaf=4)
m.fit(X_train, y_train)

print("The AUC score under the ROC curve is",roc_auc_score(y_val,m.predict_proba(X_val)[:,1] ))
sample_submission['isFraud'] = m.predict_proba(X_test)[:,1]
sample_submission.to_csv('base_RF.csv')

"""Pre-Processing Techniques used:
1. Preprocessing of columns with lot of sparse data - In case the column had more than 50% of the data as missing, the column was removed. 
This seemed like a better heuristic than trying to fill the missing columns as a large number of the rows are empty so applying any other kind of heuristic might increase the bias towards some other category.

2.Preprocessing of Numeric data where values are missing. - In this case, the null values where replaced with the median of the value in that column. This heuristic might slightly increase the bias, but it is better than filling with outlier data. 



Description of techniques used:

1) Label Encoding - This encodes or enumerates the classes between 0 to no_of_classes -1. 
This is particularly useful to convert categorical data to numeric. Another popular technique called one hot encoding can also be used, but since one hot encoding increases the dimensionality of the training set by polynomial times, I did not prefer to use it. 


ML Technique used:
1. Random Forest Classidier - 

Why ? 

Randomforest is a very good estimator for large data and uses averaging to improve the preditive accuracy and control over-fitting. 

Random forests is a set of large number of decesion trees together and the error of each will average out. 

This is also called Bagging technique.

## Part 7 - Final Result

Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. Be sure to provide a link to your Kaggle profile. Make sure to include a screenshot of your ranking. Make sure your profile includes your face and affiliation with SBU.

Kaggle Link: https://www.kaggle.com/bsrinidhibhat

Highest Rank: Rank 4665

Score: Score refers to the one I obtained on the local Test Data set. 

-> AUC score under the ROC curve is 0.9308484849083978

Number of entries: 5 (Over the period of the competation).

INCLUDE IMAGE OF YOUR KAGGLE RANKING!

![](https://drive.google.com/open?id=1ai4ym-gutFH4a3yXT1DZ9Ggamt_M4pLI)

# References

The following sites and articles were used by me in this project to understand and design data science pipeline. 

**Articles :**

1) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 

2) https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/

3) https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf

4) https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf

5) https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Seaborn_Cheat_Sheet.pdf

6) https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621




**Youtube Videos**: 

1) https://www.youtube.com/watch?v=D_2LkhMJcfY&t=203s

2) https://www.youtube.com/watch?v=IffHV_AS_Do

3) https://www.youtube.com/watch?v=loNcrMjYh64&t=43s



**Kaggle Notebook** - 

1) https://www.kaggle.com/yuewangmoophy/fraud-detection-random-forest

2) https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-633734

3) https://www.kaggle.com/alexandrerays/simple-logistic-regression-baseline



Note : The following sources were used for only to understand pipelines, technologies and to understand the apporach of different users. 

It helped me understand why the algorithm was used and different approaches. 

**Text Books**:

1) The Data Science Design Manual - Steven S Skienna

2) Python for Data Analysis - O'rielly Publications
"""